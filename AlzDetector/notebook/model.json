{
	"name": "model",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "alzspark02",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c3e48ce6-b8a4-4c09-8445-7d7dad2689b4"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/90cc145c-b084-4cc1-8094-58c0d8de0332/resourceGroups/alzdetector/providers/Microsoft.Synapse/workspaces/alzheimer/bigDataPools/alzspark02",
				"name": "alzspark02",
				"type": "Spark",
				"endpoint": "https://alzheimer.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/alzspark02",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# TODO: Join train and test for easier preprocessing\r\n",
					"\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"STORAGE_ACCOUNT = \"alzdetectorstorage\"\r\n",
					"CONTAINER_NAME = \"alzheimer-anly\"\r\n",
					"STORAGE_KEY = \"\"\r\n",
					"\r\n",
					"spark = SparkSession.builder.getOrCreate()\r\n",
					"spark.conf.set(f\"fs.azure.account.key.{STORAGE_ACCOUNT}.blob.core.windows.net\", STORAGE_KEY)"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Our first concern is with defining a proper schema for the thousands of samples in this dataset. For this analysis' purpose, there are too many columns to treat them all with care. As such, all of them, save for the `cpg_site`, will receive a `sample_id` in the form \"0000**N**\", where **N** is their identification digit."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\r\n",
					"\r\n",
					"TRAIN_SAMPLE_SIZE = 8233 \r\n",
					"TEST_SAMPLE_SIZE = 2063\r\n",
					"\r\n",
					"cpg_site_field = [StructField(\"cpg_site\", StringType(), False)] # cpg_site is the only non-double column\r\n",
					"\r\n",
					"# Name all columns for train and test set\r\n",
					"train_samples = [StructField(f\"{i:05}\", DoubleType(), True) for i in range(1, TRAIN_SAMPLE_SIZE + 1)]\r\n",
					"test_samples = [StructField(f\"{i:05}\", DoubleType(), True) for i in range(TRAIN_SAMPLE_SIZE + 1, TRAIN_SAMPLE_SIZE + TEST_SAMPLE_SIZE + 1)]\r\n",
					"\r\n",
					"train_schema = StructType(cpg_site_field + train_samples)\r\n",
					"test_schema = StructType(cpg_site_field + test_samples)\r\n",
					"\r\n",
					"train_map_info = [StructField(\"row_num\", StringType(), False),\r\n",
					"                  StructField(\"sample_id\", StringType(), False),\r\n",
					"                  StructField(\"age\", IntegerType(), False), \r\n",
					"                  StructField(\"gender\", StringType(), False),\r\n",
					"                  StructField(\"sample_type\", StringType(), False),\r\n",
					"                  StructField(\"disease\", StringType(), False)]\r\n",
					"                  \r\n",
					"test_map_info = [StructField(\"row_num\", IntegerType(), False),\r\n",
					"                 StructField(\"sample_id\", StringType(), False),\r\n",
					"                 StructField(\"gender\", StringType(), False)]\r\n",
					"\r\n",
					"train_map_schema = StructType(train_map_info)\r\n",
					"test_map_schema = StructType(test_map_info)"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"train_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/data/train.csv\"\r\n",
					"train = spark.read.csv(train_path, schema=train_schema, header=True)\r\n",
					"\r\n",
					"train_map_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/maps/trainmap.csv\"\r\n",
					"train_map = spark.read.csv(train_map_path, schema=train_map_schema, header=True)\r\n",
					"\r\n",
					"test_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/data/test.csv\"\r\n",
					"test = spark.read.csv(test_path, schema=test_schema, header=True)\r\n",
					"\r\n",
					"test_map_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/maps/testmap.csv\"\r\n",
					"test_map = spark.read.csv(test_map_path, schema=test_map_schema, header=True)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"The `train_map` and `test_map` are pretty much the labels after we're done with our model-making. We'll define them here and set them aside for later."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import format_string, monotonically_increasing_id, udf\r\n",
					"from pyspark.sql.types import BooleanType\r\n",
					"\r\n",
					"# Unneeded feature\r\n",
					"train_map = train_map.drop(\"row_num\")\r\n",
					"test_map = test_map.drop(\"row_num\")\r\n",
					"\r\n",
					"# Reformat the sample_id to match the columns for our train and test data\r\n",
					"train_map = train_map.withColumn(\"sample_id\", format_string(\"%05d\", monotonically_increasing_id()))\r\n",
					"test_map = test_map.withColumn(\"sample_id\", format_string(\"%05d\", TRAIN_SAMPLE_SIZE + 1 + monotonically_increasing_id()))\r\n",
					"\r\n",
					"def has_alzheimer(disease):\r\n",
					"    \"\"\"\r\n",
					"    Custom logic to binarize the \r\n",
					"    disease feature. \r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    if disease == \"Alzheimer's disease\":\r\n",
					"        return True\r\n",
					"    return False \r\n",
					"\r\n",
					"alzheimer_udf = udf(has_alzheimer, BooleanType()) # Must be done for pyspark to recognize the function\r\n",
					"train_map = train_map.withColumn(\"has_alzheimer\", alzheimer_udf(train_map['disease']))"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"train = train.select(train.columns[:100]).limit(5)\r\n",
					"test = test.select(test.columns[:100]).limit(5)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"To begin preprocessing, we must transpose the dataframes. Unpivoting takes our dataframe, which is originally of shape (485512, SAMPLES), and shrinks it down to a three-column spread of the id, variable column, and value column. Of course, this is a temporary measure. This allows us to create the `sample_id` column, which we will then use to re-pivot our data back into wide format--only now, our columns are the `cpg_site` and the IDs are those found in `sample_id`. If our previous shape was (485512, SAMPLES), it is now (SAMPLES, 485512)."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"NUM_CPG_SITES = 485_512\r\n",
					"spark.conf.set(\"spark.sql.pivotMaxValues\", NUM_CPG_SITES)\r\n",
					"\r\n",
					"unpivot_params = {\r\n",
					"    'ids': 'cpg_site',\r\n",
					"    'values': None,\r\n",
					"    'variableColumnName': 'sample_id',\r\n",
					"    'valueColumnName': 'methyl_data'\r\n",
					"}\r\n",
					"\r\n",
					"train_T = (train.unpivot(**unpivot_params)\r\n",
					"                .groupby('sample_id')\r\n",
					"                .pivot('cpg_site')\r\n",
					"                .agg({'methyl_data': 'last'}))\r\n",
					"\r\n",
					"test_T = (test.unpivot(**unpivot_params)\r\n",
					"              .groupby('sample_id')\r\n",
					"              .pivot('cpg_site')\r\n",
					"              .agg({'methyl_data': 'last'}))"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"The transposition is complete, but there are a lot of missing values. A pretty common tactic is to impute them with the mean, and that's what we'll do. "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.ml.feature import Imputer \r\n",
					"\r\n",
					"train_numeric_cols = train_T.columns[1:] # the first column is the ID column\r\n",
					"test_numeric_cols = test_T.columns[1:] # the first column is the ID column\r\n",
					"\r\n",
					"imputer = Imputer(inputCols=train_numeric_cols, outputCols=train_numeric_cols)\r\n",
					"model = imputer.fit(train_T)\r\n",
					"train_imputed = model.transform(train_T)\r\n",
					"\r\n",
					"imputer = Imputer(inputCols=test_numeric_cols, outputCols=test_numeric_cols)\r\n",
					"model = imputer.fit(test_T)\r\n",
					"test_imputed = model.transform(test_T)"
				],
				"execution_count": 10
			}
		]
	}
}