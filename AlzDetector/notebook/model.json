{
	"name": "model",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "alzspark02",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e479c5f0-f946-4ab8-869b-638f1c91b463"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/90cc145c-b084-4cc1-8094-58c0d8de0332/resourceGroups/alzdetector/providers/Microsoft.Synapse/workspaces/alzheimer/bigDataPools/alzspark02",
				"name": "alzspark02",
				"type": "Spark",
				"endpoint": "https://alzheimer.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/alzspark02",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"STORAGE_ACCOUNT = \"alzdetectorstorage\"\r\n",
					"CONTAINER_NAME = \"alzheimer-anly\"\r\n",
					"STORAGE_KEY = \"\"\r\n",
					"\r\n",
					"spark = SparkSession.builder.getOrCreate()\r\n",
					"spark.conf.set(f\"fs.azure.account.key.{STORAGE_ACCOUNT}.blob.core.windows.net\", STORAGE_KEY)"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Our first concern is with defining a proper schema for the thousands of samples in this dataset. For this analysis' purpose, there are too many columns to treat them all with care. As such, all of them, save for the `cpg_site`, will receive a `sample_id` in the form \"0000**N**\", where **N** is their identification digit."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\r\n",
					"\r\n",
					"TRAIN_COL_SIZE = 8233 \r\n",
					"TEST_COL_SIZE = 2063\r\n",
					"\r\n",
					"cpg_site_field = [StructField(\"cpg_site\", StringType(), False)] # cpg_site is the only non-double column\r\n",
					"\r\n",
					"# Name all columns for train and test set\r\n",
					"train_samples = [StructField(f\"{i:05}\", DoubleType(), True) for i in range(1, TRAIN_COL_SIZE + 1)]\r\n",
					"test_samples = [StructField(f\"{i:05}\", DoubleType(), True) for i in range(TRAIN_COL_SIZE + 1, TRAIN_COL_SIZE + TEST_COL_SIZE + 1)]\r\n",
					"\r\n",
					"train_schema = StructType(cpg_site_field + train_samples)\r\n",
					"test_schema = StructType(cpg_site_field + test_samples)\r\n",
					"\r\n",
					"train_map_info = [StructField(\"row_num\", StringType(), False),\r\n",
					"                  StructField(\"sample_id\", StringType(), False),\r\n",
					"                  StructField(\"age\", IntegerType(), False), \r\n",
					"                  StructField(\"gender\", StringType(), False),\r\n",
					"                  StructField(\"sample_type\", StringType(), False),\r\n",
					"                  StructField(\"disease\", StringType(), False)]\r\n",
					"                  \r\n",
					"test_map_info = [StructField(\"row_num\", IntegerType(), False),\r\n",
					"                 StructField(\"sample_id\", StringType(), False),\r\n",
					"                 StructField(\"gender\", StringType(), False)]\r\n",
					"\r\n",
					"train_map_schema = StructType(train_map_info)\r\n",
					"test_map_schema = StructType(test_map_info)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"train_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/data/train.csv\"\r\n",
					"train = spark.read.csv(train_path, schema=train_schema, header=True)\r\n",
					"\r\n",
					"train_map_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/maps/trainmap.csv\"\r\n",
					"train_map = spark.read.csv(train_map_path, schema=train_map_schema, header=True)\r\n",
					"\r\n",
					"test_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/data/test.csv\"\r\n",
					"test = spark.read.csv(test_path, schema=test_schema, header=True)\r\n",
					"\r\n",
					"test_map_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/maps/testmap.csv\"\r\n",
					"test_map = spark.read.csv(test_map_path, schema=test_map_schema, header=True)"
				],
				"execution_count": 17
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"The `train_map` and `test_map` are pretty much the labels after we're done with our model-making. We'll define them here and set them aside for later."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import format_string, monotonically_increasing_id, udf\r\n",
					"from pyspark.sql.types import BooleanType\r\n",
					"\r\n",
					"# Unneeded feature\r\n",
					"train_map = train_map.drop(\"row_num\")\r\n",
					"test_map = test_map.drop(\"row_num\")\r\n",
					"\r\n",
					"# Reformat the sample_id to match the columns for our train and test data\r\n",
					"train_map = train_map.withColumn(\"sample_id\", format_string(\"%05d\", monotonically_increasing_id()))\r\n",
					"test_map = test_map.withColumn(\"sample_id\", format_string(\"%05d\", TRAIN_COL_SIZE + 1 + monotonically_increasing_id()))\r\n",
					"\r\n",
					"def has_alzheimer(disease):\r\n",
					"    \"\"\"\r\n",
					"    Custom logic to binarize the \r\n",
					"    disease feature. \r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    if disease == \"Alzheimer's disease\":\r\n",
					"        return True\r\n",
					"    return False \r\n",
					"\r\n",
					"alzheimer_udf = udf(has_alzheimer, BooleanType()) # Must be done for pyspark to recognize the function\r\n",
					"train_map = train_map.withColumn(\"has_alzheimer\", alzheimer_udf(train_map['disease']))"
				],
				"execution_count": 35
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Let's drop the `cpg_site` feature and store it as a variable instead, as it's practically just another ID as far as DNA methylation goesâ€”useless for predictions, but useful for the final stage."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"cpg_site = train.select(\"cpg_site\")\r\n",
					"\r\n",
					"train = train.drop(\"cpg_site\")\r\n",
					"test = test.drop(\"cpg_site\")"
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"train_subset = train.select(train.columns[:300])\r\n",
					"test_subset = test.select(test.columns[:300])"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.ml.feature import Imputer \r\n",
					"\r\n",
					"train_numeric_cols = train_subset.columns\r\n",
					"train_imputer = Imputer(strategy='mean', inputCols=train_numeric_cols, outputCols=train_numeric_cols)\r\n",
					"train_model = train_imputer.fit(train_subset)\r\n",
					"\r\n",
					"test_numeric_cols = test_subset.columns\r\n",
					"test_imputer = Imputer(strategy='mean', inputCols=test_numeric_cols, outputCols=test_numeric_cols)\r\n",
					"test_model  = test_imputer.fit(test_subset)\r\n",
					"\r\n",
					"train_subset = train_model.transform(train_subset)\r\n",
					"test_subset = test_model.transform(test_subset)"
				],
				"execution_count": 24
			}
		]
	}
}