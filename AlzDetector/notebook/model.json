{
	"name": "model",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "alzspark02",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b651cb23-cf59-4647-a73d-81f409e9088d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/90cc145c-b084-4cc1-8094-58c0d8de0332/resourceGroups/alzdetector/providers/Microsoft.Synapse/workspaces/alzheimer/bigDataPools/alzspark02",
				"name": "alzspark02",
				"type": "Spark",
				"endpoint": "https://alzheimer.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/alzspark02",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# TODO: Join train and test for easier preprocessing\r\n",
					"\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"STORAGE_ACCOUNT = \"alzdetectorstorage\"\r\n",
					"CONTAINER_NAME = \"alzheimer-anly\"\r\n",
					"STORAGE_KEY = \"\"\r\n",
					"\r\n",
					"spark = SparkSession.builder.getOrCreate()\r\n",
					"spark.conf.set(f\"fs.azure.account.key.{STORAGE_ACCOUNT}.blob.core.windows.net\", STORAGE_KEY)"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Our first concern is with defining a proper schema for the thousands of samples in this dataset. For this analysis' purpose, there are too many columns to treat them all with care. As such, all of them, save for the `cpg_site`, will receive a `sample_id` in the form \"0000**N**\", where **N** is their identification digit."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\r\n",
					"\r\n",
					"TRAIN_SAMPLE_SIZE = 8233 \r\n",
					"TEST_SAMPLE_SIZE = 2063\r\n",
					"\r\n",
					"cpg_site_field = [StructField(\"cpg_site\", StringType(), False)] # cpg_site is the only non-double column\r\n",
					"\r\n",
					"# Name all columns for train and test set\r\n",
					"train_samples = [StructField(f\"{i:05}\", DoubleType(), True) for i in range(1, TRAIN_SAMPLE_SIZE + 1)]\r\n",
					"test_samples = [StructField(f\"{i:05}\", DoubleType(), True) for i in range(TRAIN_SAMPLE_SIZE + 1, TRAIN_SAMPLE_SIZE + TEST_SAMPLE_SIZE + 1)]\r\n",
					"\r\n",
					"train_schema = StructType(cpg_site_field + train_samples)\r\n",
					"test_schema = StructType(cpg_site_field + test_samples)\r\n",
					"\r\n",
					"train_map_info = [StructField(\"row_num\", StringType(), False),\r\n",
					"                  StructField(\"patient_id\", StringType(), False),\r\n",
					"                  StructField(\"age\", IntegerType(), False), \r\n",
					"                  StructField(\"gender\", StringType(), False),\r\n",
					"                  StructField(\"sample_type\", StringType(), False),\r\n",
					"                  StructField(\"disease\", StringType(), False)]\r\n",
					"                  \r\n",
					"test_map_info = [StructField(\"row_num\", IntegerType(), False),\r\n",
					"                 StructField(\"patient_id\", StringType(), False),\r\n",
					"                 StructField(\"gender\", StringType(), False)]\r\n",
					"\r\n",
					"train_map_schema = StructType(train_map_info)\r\n",
					"test_map_schema = StructType(test_map_info)"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"train_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/data/train.csv\"\r\n",
					"train = spark.read.csv(train_path, schema=train_schema, header=True)\r\n",
					"\r\n",
					"train_map_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/maps/trainmap.csv\"\r\n",
					"train_map = spark.read.csv(train_map_path, schema=train_map_schema, header=True)\r\n",
					"\r\n",
					"test_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/data/test.csv\"\r\n",
					"test = spark.read.csv(test_path, schema=test_schema, header=True)\r\n",
					"\r\n",
					"test_map_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/maps/testmap.csv\"\r\n",
					"test_map = spark.read.csv(test_map_path, schema=test_map_schema, header=True)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"The `train_map` and `test_map` are pretty much the labels after we're done with our model-making. We'll define them here and set them aside for later."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import format_string, monotonically_increasing_id, udf\r\n",
					"from pyspark.sql.types import IntegerType\r\n",
					"\r\n",
					"# Unneeded feature\r\n",
					"train_map = train_map.drop(\"row_num\")\r\n",
					"test_map = test_map.drop(\"row_num\")\r\n",
					"\r\n",
					"# Reformat the sample_id to match the columns for our train and test data\r\n",
					"train_map = train_map.withColumn(\"patient_id\", format_string(\"%05d\", monotonically_increasing_id()))\r\n",
					"test_map = test_map.withColumn(\"patient_id\", format_string(\"%05d\", TRAIN_SAMPLE_SIZE + 1 + monotonically_increasing_id()))\r\n",
					"\r\n",
					"def has_alzheimer(disease):\r\n",
					"    \"\"\"\r\n",
					"    Custom logic to binarize the \r\n",
					"    disease feature. \r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    if disease == \"Alzheimer's disease\":\r\n",
					"        return 1\r\n",
					"    return 0 \r\n",
					"\r\n",
					"alzheimer_udf = udf(has_alzheimer, IntegerType()) # Must be done for pyspark to recognize the function\r\n",
					"train_map = train_map.withColumn(\"has_alzheimer\", alzheimer_udf(train_map['disease']))"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# train = train.select(train.columns[:2000]).limit(500)\r\n",
					"# test = test.select(test.columns[:2000]).limit(500)"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"To begin preprocessing, we must transpose the dataframes. Unpivoting takes our dataframe, which is originally of shape (485512, SAMPLES), and shrinks it down to a three-column spread of the id, variable column, and value column. Of course, this is a temporary measure. This allows us to create the `sample_id` column, which we will then use to re-pivot our data back into wide format--only now, our columns are the `cpg_site` and the IDs are those found in `sample_id`. If our previous shape was (485512, SAMPLES), it is now (SAMPLES, 485512)."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"NUM_CPG_SITES = 485_512\r\n",
					"spark.conf.set(\"spark.sql.pivotMaxValues\", NUM_CPG_SITES)\r\n",
					"\r\n",
					"unpivot_params = {\r\n",
					"    'ids': 'cpg_site',\r\n",
					"    'values': None,\r\n",
					"    'variableColumnName': 'patient_id',\r\n",
					"    'valueColumnName': 'methyl_data'\r\n",
					"}\r\n",
					"\r\n",
					"train_T = (train.unpivot(**unpivot_params)\r\n",
					"                .groupby('patient_id')\r\n",
					"                .pivot('cpg_site')\r\n",
					"                .agg({'methyl_data': 'last'}))\r\n",
					"\r\n",
					"test_T = (test.unpivot(**unpivot_params)\r\n",
					"              .groupby('patient_id')\r\n",
					"              .pivot('cpg_site')\r\n",
					"              .agg({'methyl_data': 'last'}))"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"The transposition is complete, but there are a lot of missing values. A pretty common tactic is to impute them with the mean, and that's what we'll do. However, it turns out that some columns present in the train set differ from those present in the test set. We'll do some quick preprocessing, and then, we'll impute what we can."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Store the IDs\r\n",
					"train_ids = train_T.select('patient_id')\r\n",
					"test_ids = test_T.select('patient_id') \r\n",
					"\r\n",
					"train_T = train_T.drop('patient_id') \r\n",
					"test_T = test_T.drop('patient_id') "
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import re \r\n",
					"\r\n",
					"def get_chromosomes(cpg_sites): \r\n",
					"    pattern = 'ch'\r\n",
					"    chromosomes = set()\r\n",
					"\r\n",
					"    for cpg_site in cpg_sites: \r\n",
					"        if re.match(pattern, cpg_site):\r\n",
					"            chromosomes.add(cpg_site)\r\n",
					"\r\n",
					"    return chromosomes\r\n",
					"\r\n",
					"train_cols = set(train_T.columns)  \r\n",
					"chromosomes = get_chromosomes(train_cols)\r\n",
					"\r\n",
					"features = list(train_cols - chromosomes)"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.ml.feature import Imputer \r\n",
					"\r\n",
					"imputer = Imputer(inputCols=features, outputCols=features)\r\n",
					"model = imputer.fit(train_T)\r\n",
					"\r\n",
					"train_imputed = model.transform(train_T)\r\n",
					"test_imputed = model.transform(test_T)"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.ml.classification import LinearSVC\r\n",
					"from pyspark.ml.feature import VectorAssembler\r\n",
					"\r\n",
					"X = train_imputed\r\n",
					"y = train_map.select('sample_id', 'has_alzheimer')\r\n",
					"train_data = X.join(y, on='sample_id', how='inner')\r\n",
					"features = train_data.columns[1:-1]\r\n",
					"\r\n",
					"assembler = VectorAssembler(inputCols=features, outputCol=\"cpg_sites\")\r\n",
					"train_data = assembler.transform(train_data)\r\n",
					"\r\n",
					"lsvc = LinearSVC(featuresCol=\"cpg_sites\", labelCol=\"has_alzheimer\", maxIter=10)\r\n",
					"model = lsvc.fit(train_data)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"y_pred = model.transform(train_data)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"y_pred.show()"
				],
				"execution_count": 10
			}
		]
	}
}