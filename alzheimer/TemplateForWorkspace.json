{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "alzheimer"
		},
		"alzheimer-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'alzheimer-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:alzheimer.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"alzheimer-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://alzdetectorstorage.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/alzheimer-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('alzheimer-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/alzheimer-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('alzheimer-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/model')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "alzspark02",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e6b36383-6c16-457b-afef-c77064512440"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/90cc145c-b084-4cc1-8094-58c0d8de0332/resourceGroups/alzdetector/providers/Microsoft.Synapse/workspaces/alzheimer/bigDataPools/alzspark02",
						"name": "alzspark02",
						"type": "Spark",
						"endpoint": "https://alzheimer.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/alzspark02",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# TODO: Join train and test for easier preprocessing\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"STORAGE_ACCOUNT = \"alzdetectorstorage\"\r\n",
							"CONTAINER_NAME = \"alzheimer-anly\"\r\n",
							"STORAGE_KEY = \"\"\r\n",
							"\r\n",
							"spark = SparkSession.builder.getOrCreate()\r\n",
							"spark.conf.set(f\"fs.azure.account.key.{STORAGE_ACCOUNT}.blob.core.windows.net\", STORAGE_KEY)"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Our first concern is with defining a proper schema for the thousands of samples in this dataset. For this analysis' purpose, there are too many columns to treat them all with care. As such, all of them, save for the `cpg_site`, will receive a `sample_id` in the form \"0000**N**\", where **N** is their identification digit."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\r\n",
							"\r\n",
							"TRAIN_SAMPLE_SIZE = 8233 \r\n",
							"TEST_SAMPLE_SIZE = 2063\r\n",
							"\r\n",
							"cpg_site_field = [StructField(\"cpg_site\", StringType(), False)] # cpg_site is the only non-double column\r\n",
							"\r\n",
							"# Name all columns for train and test set\r\n",
							"train_samples = [StructField(f\"{i:05}\", DoubleType(), True) for i in range(1, TRAIN_SAMPLE_SIZE + 1)]\r\n",
							"test_samples = [StructField(f\"{i:05}\", DoubleType(), True) for i in range(TRAIN_SAMPLE_SIZE + 1, TRAIN_SAMPLE_SIZE + TEST_SAMPLE_SIZE + 1)]\r\n",
							"\r\n",
							"train_schema = StructType(cpg_site_field + train_samples)\r\n",
							"test_schema = StructType(cpg_site_field + test_samples)\r\n",
							"\r\n",
							"train_map_info = [StructField(\"row_num\", StringType(), False),\r\n",
							"                  StructField(\"patient_id\", StringType(), False),\r\n",
							"                  StructField(\"age\", IntegerType(), False), \r\n",
							"                  StructField(\"gender\", StringType(), False),\r\n",
							"                  StructField(\"sample_type\", StringType(), False),\r\n",
							"                  StructField(\"disease\", StringType(), False)]\r\n",
							"                  \r\n",
							"test_map_info = [StructField(\"row_num\", IntegerType(), False),\r\n",
							"                 StructField(\"patient_id\", StringType(), False),\r\n",
							"                 StructField(\"gender\", StringType(), False)]\r\n",
							"\r\n",
							"train_map_schema = StructType(train_map_info)\r\n",
							"test_map_schema = StructType(test_map_info)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"train_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/data/train.csv\"\r\n",
							"train = spark.read.csv(train_path, schema=train_schema, header=True)\r\n",
							"\r\n",
							"train_map_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/maps/trainmap.csv\"\r\n",
							"train_map = spark.read.csv(train_map_path, schema=train_map_schema, header=True)\r\n",
							"\r\n",
							"test_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/data/test.csv\"\r\n",
							"test = spark.read.csv(test_path, schema=test_schema, header=True)\r\n",
							"\r\n",
							"test_map_path = f\"wasbs://{CONTAINER_NAME}@{STORAGE_ACCOUNT}.blob.core.windows.net/maps/testmap.csv\"\r\n",
							"test_map = spark.read.csv(test_map_path, schema=test_map_schema, header=True)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The `train_map` and `test_map` are pretty much the labels after we're done with our model-making. We'll define them here and set them aside for later."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import format_string, monotonically_increasing_id, udf\r\n",
							"from pyspark.sql.types import IntegerType\r\n",
							"\r\n",
							"# Unneeded feature\r\n",
							"train_map = train_map.drop(\"row_num\")\r\n",
							"test_map = test_map.drop(\"row_num\")\r\n",
							"\r\n",
							"# Reformat the sample_id to match the columns for our train and test data\r\n",
							"train_map = train_map.withColumn(\"patient_id\", format_string(\"%05d\", monotonically_increasing_id()))\r\n",
							"test_map = test_map.withColumn(\"patient_id\", format_string(\"%05d\", TRAIN_SAMPLE_SIZE + 1 + monotonically_increasing_id()))\r\n",
							"\r\n",
							"def has_alzheimer(disease):\r\n",
							"    \"\"\"\r\n",
							"    Custom logic to binarize the \r\n",
							"    disease feature. \r\n",
							"    \"\"\"\r\n",
							"\r\n",
							"    if disease == \"Alzheimer's disease\":\r\n",
							"        return 1\r\n",
							"    return 0 \r\n",
							"\r\n",
							"alzheimer_udf = udf(has_alzheimer, IntegerType()) # Must be done for pyspark to recognize the function\r\n",
							"train_map = train_map.withColumn(\"has_alzheimer\", alzheimer_udf(train_map['disease']))\r\n",
							"train_map = train_map['patient_id', 'has_alzheimer']"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"train = train.limit(5)\r\n",
							"test = test.limit(5)"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"train_imputed = train.fillna(0)\r\n",
							"test_imputed = test.fillna(0)"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"To begin preprocessing, we must transpose the dataframes. Unpivoting takes our dataframe, which is originally of shape (485512, SAMPLES), and shrinks it down to a three-column spread of the id, variable column, and value column. Of course, this is a temporary measure. This allows us to create the `sample_id` column, which we will then use to re-pivot our data back into wide format--only now, our columns are the `cpg_site` and the IDs are those found in `sample_id`. If our previous shape was (485512, SAMPLES), it is now (SAMPLES, 485512)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"NUM_CPG_SITES = 485_512\r\n",
							"spark.conf.set(\"spark.sql.pivotMaxValues\", NUM_CPG_SITES)\r\n",
							"\r\n",
							"def transpose_df(df):\r\n",
							"    unpivot_params = {\r\n",
							"        'ids': 'cpg_site',\r\n",
							"        'values': None,\r\n",
							"        'variableColumnName': 'patient_id',\r\n",
							"        'valueColumnName': 'methyl_data'\r\n",
							"    }\r\n",
							"\r\n",
							"    df_imputed = (df.unpivot(**unpivot_params)\r\n",
							"                    .groupby('patient_id')\r\n",
							"                    .pivot('cpg_site')\r\n",
							"                    .agg({'methyl_data': 'last'}))\r\n",
							"\r\n",
							"    return df_imputed\r\n",
							"\r\n",
							"train_T = transpose_df(train_imputed)\r\n",
							"# test_T = transpose_df(test_imputed)"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"train_data = train_T.join(train_map, on='patient_id', how='left')"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"train_data.show()"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.ml.feature import VectorAssembler\r\n",
							"\r\n",
							"train_data = train_T.join(train_map, on='patient_id', how='left')\r\n",
							"features = train_data.columns[1:-1] # Ignore the IDs and Labels\r\n",
							"\r\n",
							"assembler = VectorAssembler(inputCols=features, outputCol=\"cpg_site\", handleInvalid=\"keep\")\r\n",
							"train_va = assembler.transform(train_data)"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"train_va.show()"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.ml.classification import LinearSVC\r\n",
							"from pyspark.ml.feature import VectorAssembler\r\n",
							"\r\n",
							"train_data = train_T.join(train_map, on='patient_id', how='left')\r\n",
							"features = train_data[1:-1] # Ignore the IDs and Labels\r\n",
							"\r\n",
							"assembler = VectorAssembler(inputCols=features, outputCol=\"cpg_sites\")\r\n",
							"train_va = assembler.transform(train_data)\r\n",
							"\r\n",
							"lsvc = LinearSVC(featuresCol=\"cpg_sites\", labelCol=\"has_alzheimer\", maxIter=10)\r\n",
							"model = lsvc.fit(train_va)"
						],
						"outputs": [],
						"execution_count": 84
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"y_pred = model.transform(train_data)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"y_pred.show()"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/alzspark01')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 5,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/alzspark02')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 5,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		}
	]
}